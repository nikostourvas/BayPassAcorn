# Snakefile

#Define the software container
singularity: "~/other_apps/bin/poolseq_tools_0.2.11.sif" # this path is on my system, you need to change it to your path

# Get list of sample names
with open("data/sample_list.txt") as f:
     SAMPLES = f.read().splitlines()

# Define number of chunks
chunks = 12

rule all:
    input:
        expand("results/{sample}_baypassSplitOut/chunk_{chunk}_DIC.out", sample=SAMPLES,
            chunk=[str(i) for i in range(1, chunks+1)])

# the above loop will always generate the same VCF file 200 times. Include the variable i in the awk command to generate 200 different VCF files
for i in {1..200}; do
    awk -v i=$i 'NR % 200 == i' data/sample.vcf > data/sample_${i}.vcf
done

# write a rule to run the above loop
rule sample_vcf:
    input:
        vcf="data/{sample}.vcf"
    output:
        vcf_sampled=expand("data/{sample}_{i}.vcf", i=range(1, 201))
    shell:
        "for i in {{1..200}}; do awk -v i=$i 'NR % 200 == i' {input.vcf} > data/{wildcards.sample}_$i.vcf; done"

# vcf to counts
rule vcf2counts:
    input:
        vcf="data/{sample}.vcf"
    output:
        counts=temp("results/{sample}_counts.txt")
    log:
        vcf2counts="logs/vcf2counts/{sample}_vcf2counts.log"
    shell:
        "bash vcf_to_counts.sh {input.vcf} > {output.counts} 2> {log.vcf2counts}"

# pool name fix - remove suffix from pool names
# If you do not to fix the pool names, you can skip this rule 
rule pool_name_fix:
    input:
        counts="results/{sample}_counts.txt"
    output:
        counts_fixed="results/{sample}_counts_fixed.txt"
    shell:
        "bash pool_name_fix.sh {input.counts} > {output.counts_fixed}"

# counts to baypass input file
# This is still a (computationally) costly trip to R, 
# although it is substantially more efficient than loading the VCF file
# to the poolfstat R package. 
# TODO: rewrite this with unix tools
rule counts2baypass:
    input:
        counts="results/{sample}_counts_fixed.txt"
    output:
        baypass="results/{sample}_geno_baypass.txt"
    script: "counts2baypass.R"

# split baypass input file into multiple chunks
rule split_baypass:
    input:
        "results/{sample}_geno_baypass.txt"
    output:
        chunks = ["results/{{sample}}_chunks/chunk_{{chunk}}".format(x+1) for x in range(chunks)]
    params:
        chunks=chunks
    shell:
        """
        split -n l/{params.chunks} --numeric-suffixes=1 \
        {input} results/{wildcards.sample}_chunks/chunk_

        # remove zero-padding in chunk names
        rename 's/chunk_0/chunk_/' results/{wildcards.sample}_chunks/chunk_0*
        """

#########################################
## Detecting outlier loci with baypass ##
#########################################

## Notes: 
## (1) This part of the code is run directly from the command line in the Terminal.
## (2)-d0yij = 1/5 of min haploid pool size; use npilot = 100 for final analysis (see the Manual for detailed parameter descriptions).
## (3) baypass is not scaling linearly with the no of threads; analyze subsets of data on single threads in parallel; pooldata.subset() function in poolfstat can be used.
## (4) Option 3: Contrast analysis is applied on the subsetted ACORN data!

## Option 1. Scanning the genome for differentiation (without covariate) 
# running core model for scanning the genome for differentiation using the XtX statistics
rule run_baypass_1:
    input:
        chunk="results/{sample}_chunks/chunk_{chunk}",
    params:
        threads=1,
        poolsizefile="data/poolsize",
        npop=40,
        d0yij=3,
        npilot=20
    output:
        dic = "results/{sample}_baypassSplitOut/chunk_{chunk}_DIC.out",
        mat_omega = "results/{sample}_baypassSplitOut/chunk_{chunk}_mat_omega.out",
        summary_beta_params = "results/{sample}_baypassSplitOut/chunk_{chunk}_summary_beta_params.out",
        summary_lda_omega = "results/{sample}_baypassSplitOut/chunk_{chunk}_summary_lda_omega.out",
        summary_pi_xtx = "results/{sample}_baypassSplitOut/chunk_{chunk}_summary_pi_xtx.out",
        summary_yij_pij = "results/{sample}_baypassSplitOut/chunk_{chunk}_summary_yij_pij.out"
    shell:
        """
        g_baypass \
        -nthreads {params.threads} \
        -npop {params.npop} -gfile {input.chunk} -poolsizefile {params.poolsizefile} \
        -d0yij {params.d0yij} -npilot {params.npilot}\
        -outprefix results/{wildcards.sample}_baypassSplitOut/chunk_{wildcards.chunk}
        """